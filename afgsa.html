<!DOCTYPE html>
<html>
  <head>
    <title>MCD via AFGSA</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="style.css">
  </head> 
<body>    
<h2><img src="header.jpg" alt="Teaser"></h2>
<main>
<h1>Monte Carlo Denoising via Auxiliary Feature Guided Self-Attention</h1>
      <section id="content">
        <h2>Authors</h2>
	<p><strong>Jiaqi Yu</strong>, South China University of Technology, China</p>
    	<p><strong><a href="https://nieyongwei.net/">Yongwei Nie</a>*</strong>, South China University of Technology, China</p>
      	<p><strong><a href="http://www.chengjianglong.com/">Chengjiang Long</a></strong>, JD Finance America Corporation, USA</p>
	<p><strong>Wenju Xu</strong>, OPPO US Research Center, InnoPeak Technology Inc, USA</p>
	<p><strong><a href="http://zhangqing-home.net/">Qing Zhang</a></strong>, Sun Yat-sen University, China</p>
	<p><strong>Guiqing Li</strong>, South China University of Technology, China</p>
      </section>

      <section id="centering">
        <h2>Abstract</h2>
        <p>While self-attention has been successfully applied in a variety of natural language processing and computer vision tasks, its application in Monte Carlo (MC) image denoising has not yet been well explored. This paper presents a self-attention based MC denoising deep learning network based on the fact that self-attention is essentially non-local means filtering in the embedding space which makes it inherently very suitable for the denoising task. Particularly, we modify the standard self-attention mechanism to an auxiliary feature guided self-attention that considers the by-products (e.g., auxiliary feature buffers) of the MC rendering process. As a critical prerequisite to fully exploit the performance of self-attention, we design a multi-scale feature extraction stage, which provides a rich set of raw features for the later self-attention module. As self-attention poses a high computational complexity, we describe several ways that accelerate it. Ablation experiments validate the necessity and effectiveness of the above design choices. Comparison experiments show that the proposed self-attention based MC denoising method outperforms the current state-of-the-art methods.</p>
      </section>

      <section id="font-family">
        <h2>Citation</h2>
        <p>If you find our work useful in your research, please consider citing:</p>
<pre>

</pre>
       </section>

      <section id="share">
        <h2>Resources</h2>
          <nav class="resources">
          <a class="resource-marksheet resource" href="https://github.com/Aatr0x13/MC-Denoising-via-Auxiliary-Feature-Guided-Self-Attention">
            <figure class="resource-icon">
              <img src="github.png" alt="Logo of Github">
            </figure>
            <p>
              <strong class="resource-name">Code</strong>
              find the code and model weights here
            </p>
          </a>

          <a class="resource-css-reference resource" href="https://github.com/Aatr0x13/MC-Denoising-via-Auxiliary-Feature-Guided-Self-Attention">
            <figure class="resource-icon">
              <img src="pdf.png" alt="Logo of PDF">
            </figure>
            <p>
              <strong class="resource-name">Paper (low res)</strong>
              download the paper here
            </p>
          </a>
        </nav>


        <p>Thanks <strong><a href="https://twitter.com/jgthms/">@jgthms</a></strong> for the template.</p>
      </section>
    </main>

<script type="text/javascript">
WebFontConfig = {
  google: {
    families: ['Roboto:300,400,500']
  }
};

(function(d) {
  var wf = d.createElement('script'), s = d.scripts[0];
  wf.src = 'https://ajax.googleapis.com/ajax/libs/webfont/1.6.16/webfont.js';
  s.parentNode.insertBefore(wf, s);
})(document);
</script>
  </body>
</html>
